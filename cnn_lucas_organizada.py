# -*- coding: utf-8 -*-
"""CNN Lucas Organizada

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1LpsILgMXG2-xJZNZ4Pcn9eX9PIcHFIKr
"""

from google.colab import drive
drive.mount('/content/drive')

!pip install pyts

import math
from sklearn.metrics import confusion_matrix, classification_report
from sklearn.metrics import confusion_matrix
from sklearn import svm

import matplotlib.pyplot as plt
from mpl_toolkits.axes_grid1 import ImageGrid
from pyts.image import GramianAngularField
from pyts.datasets import load_gunpoint
import matplotlib.pyplot as plt
from pyts.image import MarkovTransitionField
from tensorflow.keras.utils import to_categorical

import imblearn
from imblearn.over_sampling import SMOTE
from imblearn.over_sampling import RandomOverSampler
from imblearn.over_sampling import ADASYN
from imblearn.under_sampling import NearMiss
print(imblearn.__version__)

from sklearn import datasets
from sklearn.ensemble import AdaBoostClassifier,VotingClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.svm import SVC
import tensorflow as tf
from tensorflow.keras.preprocessing import image
import numpy as np
import keras
from tensorflow.keras.callbacks import EarlyStopping

import numpy as np
import matplotlib.pyplot as plt
from matplotlib.colors import ListedColormap
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.datasets import make_moons, make_circles, make_classification
from sklearn.neural_network import MLPClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
from sklearn.gaussian_process import GaussianProcessClassifier
from sklearn.gaussian_process.kernels import RBF
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis

import math
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import cmath
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from sklearn.model_selection import GridSearchCV
from sklearn.svm import SVC
from sklearn.decomposition import PCA
from sklearn.metrics import confusion_matrix, classification_report
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.metrics import confusion_matrix
import cv2

cols95 = np.arange(1, 86+2,1)
cols96 = np.arange(1, 184 +2,1)
cols97 = np.arange(1, 196 +2,1)
cols98 = np.arange(1, 74 +2,1)
cols99 = np.arange(1, 203 +2,1)
colsnormal = np.arange(1, 202 +2,1)
cols101 = np.arange(1, 78 +2,1)
cols102 = np.arange(1, 189 +2,1)
cols103 = np.arange(1,  79+2,1)
cols104 = np.arange(1, 88 +2,1)
cols105 = np.arange(1, 184 +2,1)
finalcols = np.arange(1, 1+2,1)

data95 = pd.read_csv(r'/content/drive/My Drive/Dados/PSD data/psd_95.csv',header=0, sep=';', usecols=cols95)
data95 = data95.transpose()

data96 = pd.read_csv(r'/content/drive/My Drive/Dados/PSD data/psd_96.csv',header=0, sep=';', usecols=cols96)
data96 = data96.transpose()

data103 = pd.read_csv(r'/content/drive/My Drive/Dados/PSD data/psd_103.csv',header=0, sep=';', usecols=cols103)
data103 = data103.transpose()

data104 = pd.read_csv(r'/content/drive/My Drive/Dados/PSD data/psd_104.csv',header=0, sep=';', usecols=cols104)
data104 = data104.transpose()

data98 = pd.read_csv(r'/content/drive/My Drive/Dados/PSD data/psd_98.csv',header=0, sep=';', usecols=cols98)
data98 = data98.transpose()

data102 = pd.read_csv(r'/content/drive/My Drive/Dados/PSD data/psd_102.csv',header=0, sep=';', usecols=cols102)
data102 = data102.transpose()

data99 = pd.read_csv(r'/content/drive/My Drive/Dados/PSD data/psd_99.csv',header=0, sep=';', usecols=cols99)
data99 = data99.transpose()

data101 = pd.read_csv(r'/content/drive/My Drive/Dados/PSD data/psd_101.csv',header=0, sep=';', usecols=cols101)
data101 = data101.transpose()

data97 = pd.read_csv(r'/content/drive/My Drive/Dados/PSD data/psd_97.csv',header=0, sep=';', usecols=cols97)
data97 = data97.transpose()

datanormal = pd.read_csv(r'/content/drive/My Drive/Dados/PSD data/psd_100.csv',header=0, sep=';', usecols=colsnormal)
datanormal = datanormal.transpose()

data105 = pd.read_csv(r'/content/drive/My Drive/Dados/PSD data/psd_105.csv',header=0, sep=';', usecols=cols105)
data105 = data105.transpose()

final = pd.read_csv(r'/content/drive/My Drive/Dados/results6.csv',header=0, sep=',',index_col=0)

finalpsd = pd.read_csv(r'/content/drive/My Drive/Dados/resultspsdcerto2.csv',header=0, sep=',',index_col=0)

final = np.array(final)

scaler = MinMaxScaler(feature_range=(0,4))

final = scaler.fit_transform(final)

data95 = np.array(data95)
data96 = np.array(data96)
data97 = np.array(data97)
data98 = np.array(data98)
data99 = np.array(data99)
datanormal = np.array(datanormal)
data101 = np.array(data101)
data102 = np.array(data102)
data103 = np.array(data103)
data104 = np.array(data104)
data105 = np.array(data105)

x0 = data95[:87,:] #03
x1 = data96[:185,:] #03
x2 = data97[:197,:] #05
x3 = data98[:75,:] #03
x4 = data99[:204,:] #03
x5 = datanormal[:203,:] #normal
x6 = data101[:79,:] #03
x7 = data102[:190,:] #03
x8 = data103[:80,:] #03
x9 = data104[:89,:] #03
x10 = data105[:185,:] #105


x = np.concatenate((x0,x1,x2,x3,x4,x5,x6,x7,x8,x9,x10), axis=0)

y = np.ones((1574), dtype=np.int32)

y[:87] = 1
y[87:272] = 2
y[272:469] = 3
y[469:544] = 4
y[544:748] = 5
y[748:951] = 6
y[951:1030] = 7
y[1030:1220] = 8
y[1220:1300] = 9
y[1300:1389] = 10
y[1389:1574] = 11

final95 = final[:87,:] #normal
final96 = final[87:272,:] #96
final97 = final[272:469,:] #96
final98 = final[469:544,:] #96
final99 = final[544:748,:] #96
finalnormal = final[748:951,:] #normal
final101 = final[951:1030,:] #96
final102 = final[1030:1220,:] #96
final103 = final[1220:1300,:]
final104 = final[1300:1389,:] #96
final105 = final[1389:1574,:] #105

finaltudo = np.concatenate((final95, final96, final97, final98, final99, finalnormal,final101, final102, final103, final104, final105), axis=0)

oversample = SMOTE(sampling_strategy='all', k_neighbors=3, random_state=1)

xf, y = oversample.fit_resample(finaltudo, y)

y = np.ones((1574), dtype=np.int32)

y[:87] = 1
y[87:272] = 2
y[272:469] = 3
y[469:544] = 4
y[544:748] = 5
y[748:951] = 6
y[951:1030] = 7
y[1030:1220] =8
y[1220:1300] = 9
y[1300:1389] = 10
y[1389:1574] = 11

x, y = oversample.fit_resample(x, y)

x.shape

x0f = np.concatenate((xf[:87],xf[1574:1691]))
x1f = np.concatenate((xf[87:272],xf[1691:1710]))
x2f = np.concatenate((xf[272:469],xf[1710:1717]))
x3f = np.concatenate((xf[469:544],xf[1717:1846]))
x4f = xf[544:748]
x5f = np.concatenate((xf[748:951],xf[1846:1847]))
x6f = np.concatenate((xf[951:1030],xf[1847:1972]))
x7f = np.concatenate((xf[1030:1220],xf[1972:1986]))
x8f = np.concatenate((xf[1220:1300],xf[1986:2110]))
x9f = np.concatenate((xf[1300:1389],xf[2110:2225]))
x10f = np.concatenate((xf[1389:1574],xf[2225:2244]))


classe05f = np.concatenate((x0f,x10f))
classe04f = np.concatenate((x1f,x9f))
classe03f = np.concatenate((x2f,x8f))
classe02f = np.concatenate((x3f,x7f))
classe01f = np.concatenate((x4f,x6f))
classenormalf = x5f



xSMOTEearlyf = np.concatenate((classe05f,classe04f,classe03f,classe02f,classe01f,classenormalf))

x0 = np.concatenate((x[:87],x[1574:1691]))
x1 = np.concatenate((x[87:272],x[1691:1710]))
x2 = np.concatenate((x[272:469],x[1710:1717]))
x3 = np.concatenate((x[469:544],x[1717:1846]))
x4 = x[544:748]
x5 = np.concatenate((x[748:951],x[1846:1847]))
x6 = np.concatenate((x[951:1030],x[1847:1972]))
x7 = np.concatenate((x[1030:1220],x[1972:1986]))
x8 = np.concatenate((x[1220:1300],x[1986:2110]))
x9 = np.concatenate((x[1300:1389],x[2110:2225]))
x10 = np.concatenate((x[1389:1574],x[2225:2244]))


classe05 = np.concatenate((x0,x10))
classe04 = np.concatenate((x1,x9))
classe03 = np.concatenate((x2,x8))
classe02 = np.concatenate((x3,x7))
classe01 = np.concatenate((x4,x6))
classenormal = x5



xSMOTEearly = np.concatenate((classe05,classe04,classe03,classe02,classe01,classenormal))

y = np.ones((2244), dtype=np.int32)

y[:408] = 0
y[408:816] = 1
y[816:1224] = 2
y[1224:1632] = 3
y[1632:2040] = 4
y[2040:2244] = 5

xSMOTEearlyf, y = oversample.fit_resample(xSMOTEearlyf, y)

y = np.ones((2244), dtype=np.int32)

y[:408] = 0
y[408:816] = 1
y[816:1224] = 2
y[1224:1632] = 3
y[1632:2040] = 4
y[2040:2244] = 5

xSMOTEearly, y = oversample.fit_resample(xSMOTEearly, y)

xfinal = np.concatenate((xSMOTEearly,xSMOTEearlyf, finalpsd), axis=1)

gadf = GramianAngularField(image_size=1.0, sample_range=(-1,1), method='d')
X_gadf = gadf.fit_transform(xfinal)

# Show the images for the first time series
fig = plt.figure(figsize=(26, 13))
grid = ImageGrid(fig, 111,
                 nrows_ncols=(1, 1),
                 axes_pad=0.15,
                 share_all=True,
                 cbar_location="right",
                 cbar_mode="single",
                 cbar_size="7%",
                 cbar_pad=0.3,
                 )
images = [ X_gadf[1000]]
titles = ['Gramian Angular Difference Field']
for image, title, ax in zip(images, titles, grid):
    im = ax.imshow(image)
    ax.set_title(title, fontdict={'fontsize': 30})
ax.cax.colorbar(im)
ax.cax.toggle_label(True)
plt.suptitle('Gramian Angular Fields', y=0.98, fontsize=16)
plt.show()

Imagem = X_gadf[: , : , : , np.newaxis]*[1,1,1]

Imagem.shape

X_train, X_test, y_train, y_test = train_test_split(Imagem, y, test_size=0.2, random_state=6)

classes = np.unique(y_train)
nClasses = len(classes)
print('Total number of outputs : ', nClasses)
print('Output classes : ', classes)

train_X = X_train
test_X = X_test
train_X.shape, test_X.shape

# Change the labels from categorical to one-hot encoding
train_Y_one_hot = to_categorical(y_train)
test_Y_one_hot = to_categorical(y_test)

# Display the change for category label using one-hot encoding
print('Original label:', y_train[7])
print('After conversion to one-hot:', train_Y_one_hot[7])

from sklearn.model_selection import train_test_split
train_X,valid_X,train_label,valid_label = train_test_split(train_X, train_Y_one_hot, test_size=0.15, random_state=13)

import keras
from keras.models import Sequential,Input,Model
from keras.layers import Dense, Dropout, Flatten
from keras.layers import Conv2D, MaxPooling2D, AveragePooling2D, GlobalAveragePooling2D
#from keras.layers.normalization import BatchNormalization
from keras.layers.advanced_activations import LeakyReLU
from keras.layers.advanced_activations import ReLU

batch_size = 16
epochs = 65
num_classes = 6

fashion_model = Sequential()
fashion_model.add(Conv2D(256, kernel_size=(4, 4),strides=(1,1), activation='linear',padding='same',input_shape=(37,37,3)))
fashion_model.add(LeakyReLU(alpha=0.2))
fashion_model.add(MaxPooling2D((2, 2),padding='same'))
fashion_model.add(Dropout(0.5))
fashion_model.add(Conv2D(128, kernel_size=(3, 3),strides=(1,1), activation='linear',padding='same'))
fashion_model.add(LeakyReLU(alpha=0.2))
fashion_model.add(MaxPooling2D(pool_size=(3, 3),padding='same'))
fashion_model.add(Dropout(0.5))
fashion_model.add(Conv2D(128, kernel_size=(2,2),strides=(1,1), activation='linear',padding='same'))
fashion_model.add(LeakyReLU(alpha=0.2))
fashion_model.add(MaxPooling2D(pool_size=(3, 3),padding='same'))
fashion_model.add(Dropout(0.3))
fashion_model.add(Flatten())
fashion_model.add(Dense(128, activation='linear'))
fashion_model.add(LeakyReLU(alpha=0.2))
fashion_model.add(Dropout(0.3))
fashion_model.add(Dense(num_classes, activation='softmax'))

fashion_model.compile(loss=keras.losses.categorical_crossentropy, optimizer='Adam' ,metrics=['accuracy'])

checkpoint_filepath = '/tmp/checkpoint'
model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(
    filepath=checkpoint_filepath,
    save_weights_only=False,
    monitor='loss',
    mode='min',
    save_best_only=True)

callbacks = [
             EarlyStopping(patience=200),
]

fashion_train = fashion_model.fit(train_X, train_label,callbacks=callbacks, batch_size=batch_size, epochs=epochs,verbose=1, validation_data=(valid_X,valid_label))

test_eval = fashion_model.evaluate(test_X, test_Y_one_hot, verbose=1)

print('Test loss:', test_eval[0])
print('Test accuracy:', test_eval[1])



accuracy = fashion_train.history['accuracy']
val_accuracy = fashion_train.history['val_accuracy']
loss = fashion_train.history['loss']
val_loss = fashion_train.history['val_loss']
epochs = range(len(accuracy))
plt.plot(epochs, accuracy, 'bo', label='Training accuracy')
plt.plot(epochs, val_accuracy, 'b', label='Validation accuracy')
plt.title('Training and validation accuracy')
plt.legend()
plt.figure()
plt.plot(epochs, loss, 'bo', label='Training loss')
plt.plot(epochs, val_loss, 'b', label='Validation loss')
plt.title('Training and validation loss')
plt.legend()
plt.show()

oxi = np.concatenate((xrandomearly, finalpsd), axis=1)

X_train, X_test, y_train, y_test = train_test_split(xSMOTEearly, y, test_size=0.2, random_state=6)

"""**Classificadores Para Comparação**"""

names = ["Nearest Neighbors", "Linear SVM", "RBF SVM",
         "Decision Tree", "Random Forest", "Neural Net", "AdaBoost",
         "Naive Bayes", "QDA"]

classifiers = [
    KNeighborsClassifier(),
    SVC(kernel="linear",random_state=1),
    SVC(gamma=0.005,random_state=1),
    #GaussianProcessClassifier(1.0 * RBF(1.0)),
    DecisionTreeClassifier(max_depth=5, random_state=1),
    RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1, random_state=1),
    MLPClassifier(alpha=1, max_iter=1000, random_state=1),
    AdaBoostClassifier(random_state=1),
    GaussianNB(),
    QuadraticDiscriminantAnalysis()]

# iterate over classifiers
for name, clf in zip(names, classifiers):
    clf.fit(X_train, y_train)
    score = clf.score(X_test, y_test)
    print("Name: "+name+" Score: "+str(score))

import sklearn.metrics as metrics

y_pred_ohe = fashion_model.predict(test_X)  # shape=(n_samples, 12)
y_pred_labels = np.argmax(y_pred_ohe, axis=1)  # only necessary if output has one-hot-encoding, shape=(n_samples)

confusion_matrix = metrics.confusion_matrix(y_true=y_test, y_pred=y_pred_labels)  # shape=(12, 12)

import math
from sklearn.metrics import confusion_matrix, classification_report
from sklearn.metrics import confusion_matrix
from sklearn import svm

#INICIO DA DEFINIÇÃO DA MATRIZ DE CONFUSAO
def plot_confusion_matrix(y_true, y_pred,classes,
    normalize=True,
    title=None,
    cmap=plt.cm.Blues):
    """
    This function prints and plots the confusion matrix.
    Normalization can be applied by setting `normalize=True`.
    """
    if not title:
        if normalize:
            title = 'Normalized confusion matrix'
        else:
            title = 'Confusion matrix, without normalization'

    # Compute confusion matrix
    cm = confusion_matrix(y_true, y_pred)
    #classes = classes[unique_labels(y_true, y_pred)]
    # Only use the labels that appear in the data
    if normalize:
        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
        print("Normalized confusion matrix")
    else:
        print('Confusion matrix, without normalization')

    print(cm)


    fig, ax = plt.subplots()
    im = ax.imshow(cm,aspect= 'auto', interpolation='nearest', cmap= cmap)


    ax.figure.colorbar(im, ax=ax)
    # We want to show all ticks...
    ax.set(xticks=np.arange(cm.shape[1]), yticks=np.arange(cm.shape[0]),
        # ... and label them with the respective list entries
        xticklabels=classes, yticklabels=classes,
        title=title,ylabel='True label', xlabel='Predicted label')

    # Rotate the tick labels and set their alignment.
    plt.setp(ax.get_xticklabels(), rotation=45, ha="right",fontsize=16,
    rotation_mode="anchor")

    # Loop over data dimensions and create text annotations.
    fmt = '0.2f' if normalize else 'd'
    thresh = cm.max() / 2.
    for i in range(cm.shape[0]):
        for j in range(cm.shape[1]):
            ax.text(j, i, format(cm[i, j], fmt),
                ha="center", va="center", fontsize=16,
                color="white" if cm[i, j] > thresh else "black")
    fig.tight_layout()

    return ax

#FIM DA DEFINIÇÃO DA MATRIZ DE CONFUSAO

plot_confusion_matrix(y_test, y_pred_labels, classes=['Balanced','1%','2%','3%','4%','5%'], normalize = True, title = 'Confusion Matrix')

plot_confusion_matrix(y_test, y_pred_labels, classes=['Balanced','3%','4%','5%'], normalize = True, title = 'Confusion Matrix')

"""#IMAGENS

"""

fig, ax = plt.subplots()

ax.plot(xfinal[1])

ax.set_xlim(-1,41)
ax.set_ylim(-5,8.5)

plt.rcParams["figure.figsize"]=(30,12)

print(xfinal[1])

from numpy import exp, abs, angle
from math import pi

scaler = MinMaxScaler(feature_range=(0,1))

x_polar = scaler.fit_transform(xfinal)

resultr = []
resultphi2 = []

for i in range(0,len(xfinal[1])):
  input_num = complex(i, xfinal[1,i]) # stored as 1+2j
  r, phi = cmath.polar(input_num)
  resultr.append(r)
  resultphi2.append(phi)

print(x_polar[1])

print(resultr)

print(resultphi)

def z2polar(z):
    return (abs(z), angle(z))

r, theta = z2polar(x[1,1])

input_num2 = complex(3, x_polar[1,3])
r, phi = cmath.polar(input_num2)

input_num2

resultphi = []
resultr = []

for i in range(0,len(x_polar[1])):
  input_num = complex(i, x_polar[1,i])
  r, phi = cmath.polar(input_num)
  #phi3 = math.acos(i/r)
  degrees = phi* 180 / pi
  resultr.append(r)
  resultphi.append(degrees)

phi3

degrees = resultphi2[1]* 180 / pi

degrees = phi3* 180 / pi

degrees

for i in range(0,len(x[1])):
    rho = np.sqrt(i**2 + x[1,i]**2)
    phi = np.arccos(x[1,i], i)
    resultrho.append(rho)
    resultphi.append(phi)
    #return(rho, phi)

dfphi = DataFrame(resultphi)

dfphi.shape

dfrho = DataFrame(resultrho)

print(r)

print(resultrho[:15])

print(resultr)

print(resultphi)

fig , ax = plt.polar(resultphi,resultr)

"""# CNN EXPERIMENTS

"""

y = np.ones((1574), dtype=np.int32)

y[:283] = 0
y[283:548] = 1
y[548:825] = 2
y[825:1099] = 3
y[1099:1371] = 4
y[1371:1574] = 5

x0 = data95[:87,:] #03
x1 = data96[:185,:] #03
x2 = data97[:197,:] #05
x3 = data98[:75,:] #03
x4 = data99[:204,:] #03
x5 = datanormal[:203,:] #normal
x6 = data101[:79,:] #03
x7 = data102[:190,:] #03
x8 = data103[:80,:] #03
x9 = data104[:89,:] #03
x10 = data105[:185,:] #105


x = np.concatenate((x0,x1,x2,x3,x4,x5,x6,x7,x8,x9,x10), axis=0)

y = np.ones((1574), dtype=np.int32)

y[:87] = 1
y[87:272] = 2
y[272:469] = 3
y[469:544] = 4
y[544:748] = 5
y[748:951] = 6
y[951:1030] = 7
y[1030:1220] = 8
y[1220:1300] = 9
y[1300:1389] = 10
y[1389:1574] = 11

classenormal = x5
classe01 = np.concatenate((x4,x6))
classe02 = np.concatenate((x3,x7))
classe03 = np.concatenate((x2,x8))
classe04 = np.concatenate((x1,x9))
classe05 = np.concatenate((x0,x10))

cnntest = np.concatenate((classe01,classe02,classe03,classe04,classe05,classenormal))

cnntest.shape